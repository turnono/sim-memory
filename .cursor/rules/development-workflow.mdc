---
description:
globs:
alwaysApply: false
---
# Development Workflow and Best Practices

## Getting Started

### Environment Setup
1. Ensure `.venv` virtual environment is activated
2. Verify Google Cloud credentials are configured
3. Check [.env](mdc:.env) file has required variables:
   - `PROJECT_ID`
   - `LOCATION` 
   - `REASONING_ENGINE_ID`
4. Run `python test_refactored_system.py` to verify setup

### Understanding the Codebase
1. Start with [sim_guide/agent.py](mdc:sim_guide/agent.py) - the root coordinator
2. Explore sub-agents in [sim_guide/sub_agents/](mdc:sim_guide/sub_agents/)
3. Review evaluation suite in [evals/](mdc:evals/) to understand expected behavior
4. Check [test_refactored_system.py](mdc:test_refactored_system.py) for comprehensive system overview

## Development Patterns

### Before Making Changes
1. **Run Tests First**: Execute `python test_refactored_system.py` to establish baseline
2. **Check Evaluations**: Run relevant evaluations from [evals/](mdc:evals/) directory
3. **Understand Architecture**: Review the affected sub-agent's structure and dependencies

### Making Changes

#### Tool Development
```bash
# 1. Locate the appropriate sub-agent
cd sim_guide/sub_agents/[agent_name]/tools/

# 2. Implement tool function with proper decorator
@tool
def your_tool_function(param: str, tool_context: ToolContext) -> str:
    """Clear description of tool purpose and usage"""
    # Implementation here

# 3. Update agent configuration to include new tool
# In agent.py: tools=[..., your_tool_function]

# 4. Update agent prompt to mention new tool
# In prompt.py: Add tool to INSTRUCTION
```

#### Service Development
```bash
# 1. Implement in appropriate services/ directory
# sim_guide/sub_agents/[agent_name]/services/your_service.py

# 2. Export in services/__init__.py
from .your_service import YourServiceClass, your_service_function

# 3. Import in tools that need the service
from ..services import your_service_function

# 4. Update tools to use the service
```

#### Agent Development
```bash
# 1. Create agent directory structure
mkdir -p sim_guide/sub_agents/new_agent/{tools,services}
touch sim_guide/sub_agents/new_agent/{__init__.py,agent.py,prompt.py}
touch sim_guide/sub_agents/new_agent/tools/__init__.py
touch sim_guide/sub_agents/new_agent/services/__init__.py

# 2. Implement agent.py with ADK patterns
# 3. Create prompt.py with DESCRIPTION and INSTRUCTION
# 4. Add tools and services as needed
# 5. Register with root agent
```

### Testing Strategy

#### Unit Testing
```bash
# Test individual tools and services
python -c "from sim_guide.sub_agents.user_context_manager.tools.memory import load_life_guidance_memory; print('✅ Import successful')"
```

#### Integration Testing
```bash
# Test sub-agent functionality
python -m evals.business_strategist_evals
python -m evals.preference_evals
```

#### System Testing
```bash
# Full system verification
python test_refactored_system.py
```

#### Evaluation Suite
```bash
# Run all evaluations (may require Google Cloud setup)
python -m evals.run_all_evals
```

### Code Review Checklist

#### For Tool Changes
- [ ] Tool has proper `@tool` decorator
- [ ] Clear docstring with parameter descriptions
- [ ] Error handling implemented
- [ ] Integration with ToolContext where needed
- [ ] Added to agent's tools list
- [ ] Mentioned in agent's prompt
- [ ] Evaluation test added or updated

#### For Service Changes
- [ ] Business logic separated from tool layer
- [ ] Async/await patterns used where appropriate
- [ ] Error handling and logging implemented
- [ ] Exported in services `__init__.py`
- [ ] Dependencies clearly documented
- [ ] Integration tests updated

#### For Agent Changes
- [ ] Follows standard directory structure
- [ ] Has dedicated prompt.py file
- [ ] Tools are properly configured
- [ ] Registered with root agent
- [ ] Evaluation tests created
- [ ] Documentation updated

#### For Prompt Changes
- [ ] Reflects actual tool capabilities
- [ ] Includes specific tool usage examples
- [ ] Aligned with agent's responsibilities
- [ ] Updated after tool modifications
- [ ] Tested with real interactions

## Quality Assurance

### Pre-commit Checks
```bash
# 1. Run system test
python test_refactored_system.py

# 2. Run relevant evaluations
python -m evals.business_strategist_evals
python -m evals.preference_evals

# 3. Check imports work correctly
python -c "from sim_guide.agent import root_agent; print('✅ Root agent import successful')"

# 4. Verify file structure integrity
ls -la sim_guide/sub_agents/*/agent.py  # Should list all sub-agents
```

### Performance Considerations
- **Memory Operations**: Test with `python -m evals.memory_behavior_evals`
- **Response Times**: Monitor evaluation execution times
- **Cost Optimization**: Verify RAG cost flags are respected
- **Scalability**: Test concurrent operations when applicable

### Error Handling Standards
```python
# Tool error handling pattern
@tool
def example_tool(param: str, tool_context: ToolContext) -> str:
    try:
        # Tool implementation
        result = process_request(param)
        return f"Success: {result}"
    except SpecificException as e:
        logger.error(f"Tool {example_tool.__name__} failed: {e}")
        return f"Error: Unable to process request. {str(e)}"
    except Exception as e:
        logger.error(f"Unexpected error in {example_tool.__name__}: {e}")
        return "Error: An unexpected error occurred. Please try again."
```

## Debugging and Troubleshooting

### Common Issues

#### Import Errors
```bash
# Check if paths are correct after refactoring
python -c "from sim_guide.sub_agents.user_context_manager.services import UserPreferences"

# Update imports to new locations
# Old: from sim_guide.services import UserPreferences  
# New: from sim_guide.sub_agents.user_context_manager.services import UserPreferences
```

#### Agent Tool Registration
```python
# Verify agent has expected tools
from sim_guide.agent import root_agent
print([tool.agent.name if hasattr(tool, 'agent') else tool.func.__name__ for tool in root_agent.tools])
```

#### Service Integration
```python
# Test service functions directly
from sim_guide.sub_agents.user_context_manager.services.rag_memory_service import health_check
import asyncio
result = asyncio.run(health_check())
print(result)
```

### Debugging Tools
- **Logging**: Check logs for error details and execution flow
- **Evaluation Results**: Use evaluation outputs to identify issues
- **System Test**: `test_refactored_system.py` provides comprehensive diagnostics
- **Mock Contexts**: Use for isolated testing of tools and services

### Performance Profiling
```python
# Time critical operations
import time
start = time.time()
# Your operation here
print(f"Operation took {time.time() - start:.2f} seconds")
```

## Deployment and Maintenance

### Environment Management
- Keep [.env](mdc:.env) file secure and up-to-date
- Verify Google Cloud credentials are properly configured
- Monitor API quotas and usage
- Regularly update dependencies

### Monitoring
- Run evaluation suite periodically
- Monitor response times and error rates
- Check memory usage and performance metrics
- Validate tool integration after changes

### Documentation Maintenance
- Update Cursor rules when architecture changes
- Keep evaluation tests current with functionality
- Document new tools and services
- Maintain prompt accuracy

## Collaboration Guidelines

### Branch Management
- Create feature branches for new agents or major changes
- Test thoroughly before merging
- Update documentation with changes
- Run full evaluation suite before merging

### Code Sharing
- Follow consistent import patterns
- Maintain clear service boundaries
- Document integration points
- Use consistent error handling patterns

### Communication
- Document architectural decisions
- Share evaluation results
- Report performance issues
- Update team on configuration changes
